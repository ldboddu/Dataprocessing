{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYvgdfW15DWr",
        "outputId": "911386b7-882e-4e30-d074-96dbc8852985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /Users/manikantaboddu/anaconda3/envs/project1/lib/python3.11/site-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /Users/manikantaboddu/anaconda3/envs/project1/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import date, timedelta\n",
        "from pyspark.sql.functions import substring, lit\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Juznamq36EJF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/09/01 13:43:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "# Creating  a Spark session\n",
        "spark = SparkSession.builder.appName(\"GenerateData\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U6tqsjYf6NG8"
      },
      "outputs": [],
      "source": [
        "# assuming some sample data  to generate data based on it\n",
        "first_names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Emily\", \"Frank\", \"Grace\", \"Henry\", \"Isabella\", \"Jack\"]\n",
        "last_names = [\"Smith\", \"Jones\", \"Williams\", \"Brown\", \"Davis\", \"Miller\", \"Wilson\", \"Moore\", \"Taylor\", \"Anderson\"]\n",
        "addresses = [\"123 Main St\", \"456 Elm St\", \"789 Oak St\", \"1011 Pine St\", \"1213 Willow St\"]\n",
        "\n",
        "# Generating  random dates of birth\n",
        "start_date = date(1970, 1, 1)\n",
        "end_date = date(2022, 12, 31)\n",
        "num_days = (end_date - start_date).days\n",
        "\n",
        "# Generating sample data\n",
        "data = []\n",
        "for _ in range(100):\n",
        "    first_name = random.choice(first_names)\n",
        "    last_name = random.choice(last_names)\n",
        "    address = random.choice(addresses)\n",
        "    random_days = random.randrange(num_days)\n",
        "    date_of_birth = start_date + timedelta(days=random_days)\n",
        "    data.append((first_name, last_name, address, date_of_birth))\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"first_name\", StringType(), True),\n",
        "    StructField(\"last_name\", StringType(), True),\n",
        "    StructField(\"address\", StringType(), True),\n",
        "    StructField(\"date_of_birth\", DateType(), True)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OI_wfZzs6TnJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/09/01 13:43:51 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# create a datafrmae and write a csv file\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.write.csv(\"generated_data.csv\", header=True, mode=\"overwrite\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk_mpDTfpc9F",
        "outputId": "da58eb94-4d34-4b69-d1ad-e84b092364b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---------+--------------+-------------+\n",
            "|first_name|last_name|       address|date_of_birth|\n",
            "+----------+---------+--------------+-------------+\n",
            "|   Charlie|   Wilson|  1011 Pine St|   2004-09-21|\n",
            "|     Grace|   Taylor|1213 Willow St|   1996-08-20|\n",
            "|     Henry|    Davis|1213 Willow St|   1996-02-07|\n",
            "|     Grace|   Wilson|    456 Elm St|   1997-06-04|\n",
            "|   Charlie|    Davis|    456 Elm St|   1990-05-09|\n",
            "|   Charlie|   Miller|1213 Willow St|   1978-10-05|\n",
            "|     Grace|   Wilson|    456 Elm St|   1995-01-14|\n",
            "|  Isabella|    Jones|1213 Willow St|   1974-07-30|\n",
            "|     Grace|    Jones|    456 Elm St|   1970-08-02|\n",
            "|     Frank| Williams|    789 Oak St|   1981-05-16|\n",
            "|     Henry|   Taylor|    456 Elm St|   2017-08-27|\n",
            "|     Frank|    Smith|   123 Main St|   2007-10-16|\n",
            "|     Grace|   Taylor|    789 Oak St|   1970-12-17|\n",
            "|     Emily| Williams|1213 Willow St|   2019-11-13|\n",
            "|     Grace|    Brown|    789 Oak St|   2000-04-21|\n",
            "|      Jack| Anderson|  1011 Pine St|   1973-05-29|\n",
            "|     David| Anderson|1213 Willow St|   1997-11-06|\n",
            "|      Jack|    Jones|    789 Oak St|   1985-09-15|\n",
            "|     Henry|    Jones|  1011 Pine St|   1992-06-22|\n",
            "|   Charlie| Anderson|  1011 Pine St|   1988-12-31|\n",
            "+----------+---------+--------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Tg2Jvw4J-g_c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "df = spark.read.csv(\"generated_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "def generate_random_string(length=8):\n",
        "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
        "\n",
        "def anonymize_first_name(name):\n",
        "    return f\"First_{generate_random_string()}\"\n",
        "\n",
        "def anonymize_last_name(name):\n",
        "    return f\"Last_{generate_random_string()}\"\n",
        "\n",
        "def anonymize_address(addr):\n",
        "    return f\"Address_{generate_random_string()}\"\n",
        "\n",
        "anonymize_first_name_udf = udf(anonymize_first_name, StringType())\n",
        "anonymize_last_name_udf = udf(anonymize_last_name, StringType())\n",
        "anonymize_address_udf = udf(anonymize_address, StringType())\n",
        "\n",
        "anonymized_df = df.withColumn(\"first_name\", anonymize_first_name_udf(col(\"first_name\"))) \\\n",
        "                 .withColumn(\"last_name\", anonymize_last_name_udf(col(\"last_name\"))) \\\n",
        "                 .withColumn(\"address\", anonymize_address_udf(col(\"address\")))\n",
        "\n",
        "anonymized_df.write.csv(\"anonymized_data.csv\", header=True, mode=\"overwrite\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOJDGolqp4Dk",
        "outputId": "6b96eb62-5a02-459c-a90e-4e05c7609af9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+-------------+----------------+-------------+\n",
            "|    first_name|    last_name|         address|date_of_birth|\n",
            "+--------------+-------------+----------------+-------------+\n",
            "|First_3xrNuzqc|Last_dlinSst9|Address_aieTJz5c|   1998-10-02|\n",
            "|First_bTM8gwFN|Last_KgRhDceQ|Address_p869COPM|   1972-08-16|\n",
            "|First_FrOu1Nla|Last_MfusHojF|Address_DgWSUTO8|   1978-10-27|\n",
            "|First_VHMTsdtE|Last_00ueD6jc|Address_YQrhjJEb|   1991-11-02|\n",
            "|First_q06gch2T|Last_ohmFD3hm|Address_Pvp9ddA7|   2020-06-14|\n",
            "|First_KTvQJjV0|Last_mNlbSJvK|Address_a12lmJMw|   1997-05-27|\n",
            "|First_4ZFe9iFF|Last_1Dt74Ggg|Address_Q622kIGM|   2011-09-07|\n",
            "|First_KfibNOix|Last_BxszeGC9|Address_G1HqDVTR|   2010-09-02|\n",
            "|First_bToyQsAj|Last_2qPxBqid|Address_aXFZY3yM|   1977-09-17|\n",
            "|First_2xCIZgM7|Last_WVHRH3Gu|Address_ZGO3hh4w|   1975-10-02|\n",
            "|First_CrYHzQZB|Last_d8kV4It2|Address_imiT85NZ|   1981-02-01|\n",
            "|First_aCm3mJho|Last_GcpSaXHR|Address_TbH8JOuZ|   1979-10-01|\n",
            "|First_LjZb1Mob|Last_hpgD0rOV|Address_ftusVWKi|   2021-12-30|\n",
            "|First_oy9UO4ye|Last_Gm5SlawG|Address_SuwHeGAH|   2004-10-22|\n",
            "|First_bvOPCse9|Last_kJpMmEaT|Address_os6ddmF3|   2002-07-02|\n",
            "|First_FmXkPVX1|Last_LoliE78E|Address_lOG8XZr7|   1974-01-14|\n",
            "|First_5uHr3uJf|Last_1VQ0kzpJ|Address_qId8RN43|   2004-09-21|\n",
            "|First_2axm5IhB|Last_ttIzRvW9|Address_p8NPB8VT|   1996-08-20|\n",
            "|First_0lrssq8P|Last_7soS6723|Address_fc2mfyhT|   1996-02-07|\n",
            "|First_juV90nj8|Last_KB0Vv7LE|Address_iABR9W9b|   1997-06-04|\n",
            "+--------------+-------------+----------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "anonymized_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trying on dataset downloaded from kaggle , and anonymize all the columns except the 'FinalGrade' column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/09/01 13:58:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"kaggle_student_dataset\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"/Users/manikantaboddu/Downloads/student_performance.csv\", header=True, inferSchema=True)\n",
        "\n",
        "def generate_random_string(length=8):\n",
        "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
        "\n",
        "\n",
        "def anonymize_column(value):\n",
        "    return f\"Anonymized_{generate_random_string()}\"\n",
        "\n",
        "\n",
        "anonymize_udf = udf(anonymize_column, StringType())\n",
        "\n",
        "# Anonymize all the fields excluding 'FinalGrade'\n",
        "columns_to_anonymize = [col_name for col_name in df.columns if col_name != 'FinalGrade']\n",
        "anonymized_df = df\n",
        "for col_name in columns_to_anonymize:\n",
        "    anonymized_df = anonymized_df.withColumn(col_name, anonymize_udf(col(col_name)))\n",
        "\n",
        "\n",
        "anonymized_df.write.csv(\"anonymized_Kaggle_student_data.csv\", header=True, mode=\"overwrite\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Insted of mention the column name in the list, used below code to anonymize all the columns except the specified columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "import random\n",
        "import string\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Anonymization\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"/Users/manikantaboddu/Downloads/student_performance.csv\", header=True, inferSchema=True)\n",
        "\n",
        "def generate_random_string(length=8):\n",
        "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
        "\n",
        "def anonymize_column(value):\n",
        "    return f\"Anonymized_{generate_random_string()}\"\n",
        "\n",
        "anonymize_udf = udf(anonymize_column, StringType())\n",
        "\n",
        "# excluding the 2nd and 3rd columns from anonymization\n",
        "columns_to_exclude = [df.columns[1], df.columns[2]]\n",
        "\n",
        "\n",
        "columns_to_anonymize = [col_name for col_name in df.columns if col_name not in columns_to_exclude]\n",
        "\n",
        "\n",
        "anonymized_df = df\n",
        "for col_name in columns_to_anonymize:\n",
        "    anonymized_df = anonymized_df.withColumn(col_name, anonymize_udf(col(col_name)))\n",
        "\n",
        "\n",
        "anonymized_df.write.csv(\"anonymized_student_data.csv\", header=True, mode=\"overwrite\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
